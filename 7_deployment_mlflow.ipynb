{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Deploy Best MLflow Model to SageMaker Endpoint\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Query MLflow tracking server to find the best performing model\n",
    "2. Deploy the best model to a SageMaker real-time endpoint\n",
    "3. Test the deployed endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade packages if needed\n",
    "# !pip install --upgrade sagemaker mlflow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Configure SageMaker Session and MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Clients\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "# S3 Bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"qwen3-0-6-lora-samples-mlflow\"\n",
    "\n",
    "print(f\"Using bucket: {bucket}\")\n",
    "print(f\"Using region: {region}\")\n",
    "print(f\"Using role: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow tracking server configuration\n",
    "# Update this ARN to match your MLflow tracking server\n",
    "mlflow_tracking_server_arn = \"YOUR_ARN_HERE\"\n",
    "mlflow_experiment_name = \"qwen3-lora-training\"\n",
    "\n",
    "print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")\n",
    "print(f\"MLflow Experiment Name: {mlflow_experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Connect to MLflow Tracking Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(mlflow_tracking_server_arn)\n",
    "print(f\"MLflow tracking URI configured: {mlflow_tracking_server_arn}\")\n",
    "\n",
    "# Get experiment\n",
    "experiment = mlflow.get_experiment_by_name(mlflow_experiment_name)\n",
    "if experiment is None:\n",
    "    raise ValueError(f\"Experiment '{mlflow_experiment_name}' not found. Please check the experiment name.\")\n",
    "\n",
    "print(f\"\\nExperiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Experiment Name: {experiment.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Search MLflow Runs to Find Best Model\n",
    "\n",
    "We'll use `mlflow.search_runs()` to find the model with the lowest evaluation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all completed runs in the experiment\n",
    "runs_df = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"status = 'FINISHED'\",\n",
    "    order_by=[\"metrics.eval_loss ASC\"],  # Sort by eval_loss in ascending order (lower is better)\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "print(f\"Found {len(runs_df)} completed runs\")\n",
    "\n",
    "# Display relevant columns\n",
    "if len(runs_df) > 0:\n",
    "    columns_to_display = [\n",
    "        'run_id', \n",
    "        'start_time',\n",
    "        'metrics.eval_loss',\n",
    "        'metrics.train_loss',\n",
    "        'params.learning_rate',\n",
    "        'params.num_train_epochs',\n",
    "        'tags.mlflow.runName'\n",
    "    ]\n",
    "    # Filter columns that exist\n",
    "    existing_columns = [col for col in columns_to_display if col in runs_df.columns]\n",
    "    display(runs_df[existing_columns].head(10))\n",
    "else:\n",
    "    print(\"No completed runs found in the experiment.\")\n",
    "    print(\"Please ensure that at least one training job has completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Select Best Model\n",
    "\n",
    "Select the model with the lowest evaluation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(runs_df) == 0:\n",
    "    raise ValueError(\"No completed runs found. Cannot proceed with deployment.\")\n",
    "\n",
    "# Get the best run (already sorted by eval_loss ascending)\n",
    "best_run = runs_df.iloc[0]\n",
    "best_run_id = best_run['run_id']\n",
    "best_eval_loss = best_run.get('metrics.eval_loss', 'N/A')\n",
    "best_train_loss = best_run.get('metrics.train_loss', 'N/A')\n",
    "\n",
    "print(\"=== Best Model Selected ===\")\n",
    "print(f\"Run ID: {best_run_id}\")\n",
    "print(f\"Run Name: {best_run.get('tags.mlflow.runName', 'N/A')}\")\n",
    "print(f\"Eval Loss: {best_eval_loss}\")\n",
    "print(f\"Train Loss: {best_train_loss}\")\n",
    "print(f\"Learning Rate: {best_run.get('params.learning_rate', 'N/A')}\")\n",
    "print(f\"Epochs: {best_run.get('params.num_train_epochs', 'N/A')}\")\n",
    "print(f\"Started: {best_run['start_time']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Get Model Artifacts from MLflow\n",
    "\n",
    "Retrieve the model artifacts path from the MLflow run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get run details\n",
    "run = mlflow.get_run(best_run_id)\n",
    "\n",
    "print(\"=== Run Details ===\")\n",
    "print(f\"Run ID: {run.info.run_id}\")\n",
    "print(f\"Status: {run.info.status}\")\n",
    "print(f\"\\nLogged Artifacts:\")\n",
    "\n",
    "# List artifacts in the run\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "artifacts = client.list_artifacts(best_run_id)\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(f\"  - {artifact.path}\")\n",
    "\n",
    "# Note: In this workflow, the model artifacts are stored in S3 by the SageMaker training job\n",
    "# We need to get the S3 model data path from the training job metadata\n",
    "# The training job name is typically stored in the run tags\n",
    "training_job_name = run.data.tags.get('mlflow.runName', '')\n",
    "print(f\"\\nTraining Job Name (from MLflow run name): {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Get Model Data S3 Path from SageMaker Training Job\n",
    "\n",
    "Since the model artifacts are stored in S3 by SageMaker, we need to retrieve the S3 path from the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll need to find the corresponding SageMaker training job\n",
    "# The training job outputs the model to S3, which we'll use for deployment\n",
    "\n",
    "# List recent training jobs to find the one matching our run\n",
    "# You may need to adjust this based on your naming convention\n",
    "output_path = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "# Get the model data path from the training job\n",
    "# Try to find the training job by searching for jobs with matching timestamps\n",
    "# Or use a stored training job name if available\n",
    "\n",
    "# Option 1: If you have the training job name from the MLflow run\n",
    "# You can extract it from tags or parameters\n",
    "\n",
    "# For now, let's list recent training jobs and let the user select\n",
    "response = sm_client.list_training_jobs(\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=20,\n",
    "    StatusEquals='Completed'\n",
    ")\n",
    "\n",
    "print(\"=== Recent Completed Training Jobs ===\")\n",
    "for idx, job in enumerate(response['TrainingJobSummaries'][:10]):\n",
    "    print(f\"{idx}: {job['TrainingJobName']} (Created: {job['CreationTime']})\")\n",
    "\n",
    "# For demonstration, let's use the most recent completed training job\n",
    "# In production, you should match the training job name from MLflow tags\n",
    "latest_training_job_name = response['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "print(f\"\\nUsing training job: {latest_training_job_name}\")\n",
    "print(\"\\nNote: In production, you should store the SageMaker training job name in MLflow tags\")\n",
    "print(\"to ensure you're deploying the correct model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training job details to retrieve model data S3 path\n",
    "training_job_details = sm_client.describe_training_job(\n",
    "    TrainingJobName=latest_training_job_name\n",
    ")\n",
    "\n",
    "model_data_s3_uri = training_job_details['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "print(\"=== Model Artifacts ===\")\n",
    "print(f\"Training Job: {latest_training_job_name}\")\n",
    "print(f\"Model Data S3 URI: {model_data_s3_uri}\")\n",
    "print(f\"Training Job Status: {training_job_details['TrainingJobStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Create SageMaker Model from Best MLflow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PyTorch inference image\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Use PyTorch 2.7.1 inference image\n",
    "pytorch_inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.7.1\",\n",
    "    py_version=\"py312\",\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f\"PyTorch Inference Image: {pytorch_inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model name with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"qwen3-mlflow-best-model-{timestamp}\"\n",
    "\n",
    "# Create model\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        'Image': pytorch_inference_image_uri,\n",
    "        'ModelDataUrl': model_data_s3_uri,\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "            'SAGEMAKER_SUBMIT_DIRECTORY': model_data_s3_uri,\n",
    "            'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
    "            'SAGEMAKER_REGION': region,\n",
    "        }\n",
    "    },\n",
    "    Tags=[\n",
    "        {'Key': 'Project', 'Value': 'MLOps-Workshop'},\n",
    "        {'Key': 'Model', 'Value': 'QWEN3-0.6B'},\n",
    "        {'Key': 'MLflowRunId', 'Value': best_run_id},\n",
    "        {'Key': 'EvalLoss', 'Value': str(best_eval_loss)}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Created ===\")\n",
    "print(f\"Model Name: {model_name}\")\n",
    "print(f\"Model ARN: {create_model_response['ModelArn']}\")\n",
    "print(f\"Model Data: {model_data_s3_uri}\")\n",
    "print(f\"MLflow Run ID: {best_run_id}\")\n",
    "print(f\"Best Eval Loss: {best_eval_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9. Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint configuration name\n",
    "endpoint_config_name = f\"qwen3-mlflow-config-{timestamp}\"\n",
    "\n",
    "# Create endpoint configuration\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': model_name,\n",
    "            'InstanceType': 'ml.g5.2xlarge',\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InitialVariantWeight': 1,\n",
    "            'ManagedInstanceScaling': {\n",
    "                'Status': 'ENABLED',\n",
    "                'MinInstanceCount': 1,\n",
    "                'MaxInstanceCount': 1\n",
    "            },\n",
    "            'RoutingConfig': {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    Tags=[\n",
    "        {'Key': 'Project', 'Value': 'MLOps-Workshop'},\n",
    "        {'Key': 'Model', 'Value': 'QWEN3-0.6B'},\n",
    "        {'Key': 'Source', 'Value': 'MLflow'},\n",
    "        {'Key': 'MLflowRunId', 'Value': best_run_id}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Endpoint Configuration Created ===\")\n",
    "print(f\"Config Name: {endpoint_config_name}\")\n",
    "print(f\"Config ARN: {endpoint_config_response['EndpointConfigArn']}\")\n",
    "print(f\"Instance Type: ml.g5.2xlarge\")\n",
    "print(f\"Initial Instance Count: 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. Create and Deploy SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint name\n",
    "endpoint_name = f\"qwen3-mlflow-endpoint-{timestamp}\"\n",
    "\n",
    "# Create endpoint\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    Tags=[\n",
    "        {'Key': 'Project', 'Value': 'MLOps-Workshop'},\n",
    "        {'Key': 'Model', 'Value': 'QWEN3-0.6B'},\n",
    "        {'Key': 'DeploymentType', 'Value': 'RealTime'},\n",
    "        {'Key': 'Source', 'Value': 'MLflow'},\n",
    "        {'Key': 'MLflowRunId', 'Value': best_run_id}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Creating Endpoint ===\")\n",
    "print(f\"Endpoint Name: {endpoint_name}\")\n",
    "print(f\"Endpoint ARN: {create_endpoint_response['EndpointArn']}\")\n",
    "print(f\"Status: Creating...\")\n",
    "print(f\"\\nThis process will take 5-10 minutes. You can monitor progress in the SageMaker console.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 11. Wait for Endpoint to be Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for endpoint to be in service\n",
    "print(\"Waiting for endpoint to be in service...\")\n",
    "\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "\n",
    "try:\n",
    "    waiter.wait(\n",
    "        EndpointName=endpoint_name,\n",
    "        WaiterConfig={\n",
    "            'Delay': 30,  # Check every 30 seconds\n",
    "            'MaxAttempts': 40  # Maximum 20 minutes\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get endpoint status\n",
    "    endpoint_response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    \n",
    "    print(\"\\n=== Endpoint Deployment Successful ===\")\n",
    "    print(f\"Endpoint Name: {endpoint_name}\")\n",
    "    print(f\"Status: {endpoint_response['EndpointStatus']}\")\n",
    "    print(f\"Created: {endpoint_response['CreationTime']}\")\n",
    "    print(f\"\\nDeployed Model Details:\")\n",
    "    print(f\"  - MLflow Run ID: {best_run_id}\")\n",
    "    print(f\"  - Eval Loss: {best_eval_loss}\")\n",
    "    print(f\"  - Model S3 URI: {model_data_s3_uri}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error waiting for endpoint: {e}\")\n",
    "    print(\"Please check the endpoint status in the SageMaker console.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 12. Test the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predictor for the endpoint\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(\"=== Testing Endpoint ===\")\n",
    "print(\"\\nNote: If you get a timeout error, the model might still be loading.\")\n",
    "print(\"Wait a few minutes and try again.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple greeting\n",
    "test_input_1 = {\n",
    "    \"inputs\": \"Hi!\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 20,\n",
    "        \"temperature\": 0.1,\n",
    "        \"do_sample\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Test 1: Simple Greeting\")\n",
    "print(f\"Input: {test_input_1['inputs']}\")\n",
    "response_1 = predictor.predict(data=test_input_1)\n",
    "print(f\"Output: {response_1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow for:\n",
    "\n",
    "1. **MLflow Integration**:\n",
    "   - Connected to SageMaker MLflow tracking server\n",
    "   - Queried experiment runs using `mlflow.search_runs()`\n",
    "   - Selected the best model based on evaluation loss\n",
    "\n",
    "2. **Model Deployment**:\n",
    "   - Retrieved model artifacts from the best MLflow run\n",
    "   - Created a SageMaker Model with proper tagging\n",
    "   - Deployed to a real-time SageMaker endpoint\n",
    "\n",
    "3. **Testing and Validation**:\n",
    "   - Tested the endpoint with sample inputs\n",
    "   - Verified model performance\n",
    "\n",
    "4. **Traceability**:\n",
    "   - Maintained full traceability from MLflow run to deployed endpoint\n",
    "   - Tagged all resources with MLflow run ID for tracking\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Automated Model Selection**: Uses MLflow to automatically find the best model\n",
    "- **Experiment Tracking**: Full visibility into model performance and hyperparameters\n",
    "- **Reproducibility**: Can always trace back to the exact training run\n",
    "- **Governance**: Clear audit trail from training to deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **A/B Testing**: Deploy multiple model versions and compare performance\n",
    "2. **Auto-Scaling**: Configure auto-scaling policies based on traffic\n",
    "3. **Model Monitoring**: Set up CloudWatch alarms for latency and errors\n",
    "4. **CI/CD Pipeline**: Automate the deployment process with SageMaker Pipelines\n",
    "5. **Model Registry**: Register the best model in MLflow Model Registry for production\n",
    "\n",
    "For more information:\n",
    "- [SageMaker MLflow Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html)\n",
    "- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)\n",
    "- [SageMaker Endpoint Deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Pipeline for End-to-End MLOps\n",
    "\n",
    "This notebook creates a SageMaker Pipeline that automates the entire ML workflow:\n",
    "1. Training with QWEN3-0.6B LoRA\n",
    "2. Model evaluation with Processing Job\n",
    "3. Conditional model registration based on performance\n",
    "4. Automatic deployment to endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.steps import (\n",
    "    TrainingStep,\n",
    "    ProcessingStep,\n",
    "    CreateModelStep,\n",
    "    CacheConfig\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo, ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterFloat,\n",
    "    ParameterString\n",
    ")\n",
    "from sagemaker.workflow.functions import Join, JsonGet\n",
    "from sagemaker.pytorch import PyTorch, PyTorchProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure SageMaker Session and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::637423390840:role/WSParticipantRole\" # need to change your role\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# S3 configuration\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"qwen3-0-6-lora-pipeline\"\n",
    "\n",
    "# Pipeline name\n",
    "pipeline_name = \"qwen3-lora-mlops-pipeline\"\n",
    "\n",
    "# Model package group name\n",
    "model_package_group_name = \"qwen3-0-6b-lora-models\"\n",
    "\n",
    "print(f\"Using bucket: {bucket}\")\n",
    "print(f\"Using prefix: {prefix}\")\n",
    "print(f\"Pipeline name: {pipeline_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline parameters - these can be overridden at runtime\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.g5.2xlarge\"  \n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "num_train_epochs = ParameterInteger(\n",
    "    name=\"NumTrainEpochs\",\n",
    "    default_value=3\n",
    ")\n",
    "\n",
    "learning_rate = ParameterFloat(\n",
    "    name=\"LearningRate\",\n",
    "    default_value=2e-4\n",
    ")\n",
    "\n",
    "# Updated thresholds for CPU-based evaluation metrics\n",
    "eval_loss_threshold = ParameterFloat(\n",
    "    name=\"EvalLossThreshold\", \n",
    "    default_value=0.5  # For derived loss from text similarity\n",
    ")\n",
    "\n",
    "bleu_score_threshold = ParameterFloat(\n",
    "    name=\"BleuScoreThreshold\",\n",
    "    default_value=0.25  # For derived BLEU score from text similarity\n",
    ")\n",
    "\n",
    "# Add text similarity threshold (primary metric for CPU evaluation)\n",
    "text_similarity_threshold = ParameterFloat(\n",
    "    name=\"TextSimilarityThreshold\",\n",
    "    default_value=0.5  # Minimum text similarity score\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"  # or \"Approved\" for auto-approval\n",
    ")\n",
    "\n",
    "print(\"Pipeline parameters defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload training and test data to S3\n",
    "train_s3_uri = sagemaker_session.upload_data(\n",
    "    path='samples/train.jsonl',\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/data/train'\n",
    ")\n",
    "\n",
    "test_s3_uri = sagemaker_session.upload_data(\n",
    "    path='samples/test.jsonl',\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/data/test'\n",
    ")\n",
    "\n",
    "print(f\"Training data: {train_s3_uri}\")\n",
    "print(f\"Test data: {test_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step 1: Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure training estimator\n",
    "train_hyperparameters = {\n",
    "    # Model\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \n",
    "    # Training\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"num_train_epochs\": num_train_epochs,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 64,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"save_steps\": 50,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_total_limit\": 3,\n",
    "    \"do_eval\": True,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 50,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"load_best_model_at_end\": False,\n",
    "    \"report_to\": \"none\",\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 4, \n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\",\n",
    "    \n",
    "    # Dataset\n",
    "    \"train_file\": \"/opt/ml/input/data/train/train.jsonl\",\n",
    "    \"validation_split_percentage\": 20,\n",
    "    \"block_size\": 256,\n",
    "}\n",
    "\n",
    "# Create PyTorch estimator with fixed versions for compatibility\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"2.6.0\", # At this time, 2.7.1 for TrainingStep is not supported\n",
    "    py_version=\"py312\", \n",
    "    hyperparameters=train_hyperparameters,\n",
    "    output_path=f\"s3://{bucket}/{prefix}/output\",\n",
    "    checkpoint_s3_uri=f\"s3://{bucket}/{prefix}/checkpoints\",\n",
    "    use_spot_instances=False,\n",
    "    max_run=24*60*60,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    volume_size=450,\n",
    "    environment={\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "\n",
    "# Create training step\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainQwen3LoRA\",\n",
    "    estimator=pytorch_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=train_s3_uri,\n",
    "            content_type=\"application/jsonl\"\n",
    "        )\n",
    "    },\n",
    "    cache_config=CacheConfig(enable_caching=True, expire_after=\"30d\")\n",
    ")\n",
    "\n",
    "print(\"Training step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step 2: Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_session = PipelineSession()\n",
    "    \n",
    "eval_processor = PyTorchProcessor(\n",
    "    framework_version=\"2.6.0\",\n",
    "    py_version=\"py312\",  \n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"qwen3-eval\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "# Define property file for evaluation metrics\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation_metrics.json\"\n",
    ")\n",
    "\n",
    "# Generate step_args using processor.run() to handle source_dir\n",
    "step_args = eval_processor.run(\n",
    "    code=\"evaluation/evaluate.py\",\n",
    "    source_dir=\"src\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=test_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"s3://{bucket}/{prefix}/evaluation\"\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Create evaluation step using step_args\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    step_args=step_args,\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=CacheConfig(enable_caching=True, expire_after=\"30d\")\n",
    ")\n",
    "\n",
    "print(\"Evaluation step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Step 3: Model Registration Step (Conditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model metrics\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(\n",
    "            on=\"/\",\n",
    "            values=[\n",
    "                step_eval.properties.ProcessingOutputConfig.Outputs[\"evaluation\"].S3Output.S3Uri,\n",
    "                \"evaluation_metrics.json\"\n",
    "            ]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get inference image\n",
    "from sagemaker import image_uris\n",
    "inference_image = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.6.0\",\n",
    "    py_version=\"py312\",\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# For pipeline, we'll use RegisterModel without custom inference script\n",
    "# The model will use the default PyTorch serving handler\n",
    "step_register = RegisterModel(\n",
    "    name=\"RegisterQwen3Model\",\n",
    "    estimator=pytorch_estimator,  # Use the training estimator\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.g5.xlarge\", \"ml.g5.2xlarge\"],\n",
    "    transform_instances=[\"ml.g5.xlarge\", \"ml.g5.2xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    "    description=\"QWEN3-0.6B LoRA fine-tuned model from pipeline\"\n",
    ")\n",
    "\n",
    "print(\"Model registration step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Step 4: Conditional Registration Based on Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create conditions for model registration based on CPU evaluation metrics\n",
    "# CPU evaluation produces: text_similarity, derived eval_loss, derived bleu_score\n",
    "\n",
    "# Condition 1: Check text similarity (primary metric)\n",
    "cond_similarity = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"text_similarity\"\n",
    "    ),\n",
    "    right=text_similarity_threshold\n",
    ")\n",
    "\n",
    "# Create condition step - using text_similarity as primary condition\n",
    "step_cond = ConditionStep(\n",
    "    name=\"CheckModelPerformance\",\n",
    "    conditions=[cond_similarity],  # Only condition: text similarity\n",
    "    if_steps=[step_register],  # Register model\n",
    "    else_steps=[]  # Do nothing if conditions not met\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create pipeline with updated parameters\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        training_instance_type,\n",
    "        processing_instance_type,\n",
    "        num_train_epochs,\n",
    "        learning_rate,\n",
    "        eval_loss_threshold,\n",
    "        bleu_score_threshold,\n",
    "        text_similarity_threshold,  # Add new parameter\n",
    "        model_approval_status\n",
    "    ],\n",
    "    steps=[step_train, step_eval, step_cond],\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Submit Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create or update pipeline\n",
    "pipeline_response = pipeline.upsert(role_arn=role)\n",
    "\n",
    "print(f\"Pipeline ARN: {pipeline_response['PipelineArn']}\")\n",
    "print(f\"\\\\n✓ Pipeline '{pipeline_name}' created/updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Start Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start pipeline execution\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "execution_name = f\"{pipeline_name}-{timestamp}\"\n",
    "\n",
    "# You can override parameters here if needed\n",
    "execution = pipeline.start(\n",
    "    execution_display_name=execution_name,\n",
    "    parameters={\n",
    "        # Override parameters if needed\n",
    "        # \"NumTrainEpochs\": 5,\n",
    "        # \"LearningRate\": 1e-4,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Pipeline execution started: {execution_name}\")\n",
    "print(f\"Execution ARN: {execution.arn}\")\n",
    "print(\"\\\\nYou can monitor the pipeline execution in the SageMaker Studio or console.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Monitor Pipeline Execution (Also you can see the progress in the SageMaker Studio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait for execution to complete (optional)\n",
    "import time\n",
    "\n",
    "print(\"Monitoring pipeline execution...\")\n",
    "print(\"This will take approximately 30-45 minutes...\\\\n\")\n",
    "\n",
    "while True:\n",
    "    status = execution.describe()['PipelineExecutionStatus']\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['Succeeded', 'Failed', 'Stopped']:\n",
    "        break\n",
    "    \n",
    "    time.sleep(60)  # Check every minute\n",
    "\n",
    "if status == 'Succeeded':\n",
    "    print(\"\\\\n✓ Pipeline execution completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\\\n✗ Pipeline execution ended with status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. List Pipeline Executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List recent pipeline executions\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "response = sm_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=5\n",
    ")\n",
    "\n",
    "print(f\"=== Recent Executions for {pipeline_name} ===\")\n",
    "for idx, exec_summary in enumerate(response['PipelineExecutionSummaries'], 1):\n",
    "    print(f\"\\\\n{idx}. Execution:\")\n",
    "    print(f\"   ARN: {exec_summary['PipelineExecutionArn']}\")\n",
    "    print(f\"   Status: {exec_summary['PipelineExecutionStatus']}\")\n",
    "    print(f\"   Start Time: {exec_summary['StartTime']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Configure Pipeline Schedule (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EventBridge rule to trigger pipeline on schedule\n",
    "# This example triggers the pipeline daily at 2 AM UTC\n",
    "\n",
    "import boto3\n",
    "events_client = boto3.client('events', region_name=region)\n",
    "\n",
    "# Create schedule rule\n",
    "rule_name = f\"{pipeline_name}-daily-schedule\"\n",
    "\n",
    "try:\n",
    "    response = events_client.put_rule(\n",
    "        Name=rule_name,\n",
    "        ScheduleExpression='cron(0 2 * * ? *)',  # Daily at 2 AM UTC\n",
    "        State='DISABLED',  # Start with disabled, enable when ready\n",
    "        Description=f'Daily trigger for {pipeline_name}'\n",
    "    )\n",
    "    \n",
    "    # Add target (SageMaker Pipeline)\n",
    "    response = events_client.put_targets(\n",
    "        Rule=rule_name,\n",
    "        Targets=[\n",
    "            {\n",
    "                'Id': '1',\n",
    "                'Arn': pipeline_response['PipelineArn'],\n",
    "                'RoleArn': role,\n",
    "                'SageMakerPipelineParameters': {\n",
    "                    'PipelineParameterList': [\n",
    "                        # Add any parameter overrides here\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Schedule rule '{rule_name}' created (currently DISABLED)\")\n",
    "    print(\"To enable the schedule, uncomment and run the next cell\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Schedule creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the schedule (uncomment to activate)\n",
    "# response = events_client.enable_rule(Name=rule_name)\n",
    "# print(f\"✓ Schedule '{rule_name}' enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Pipeline Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the pipeline when no longer needed\n",
    "# response = sm_client.delete_pipeline(\n",
    "#     PipelineName=pipeline_name\n",
    "# )\n",
    "# print(f\"Pipeline '{pipeline_name}' deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook created an end-to-end MLOps pipeline with integration:\n",
    "\n",
    "1. **Training Step**: Fine-tunes QWEN3-0.6B with LoRA on training data (GPU: ml.g5.2xlarge)\n",
    "2. **Evaluation Step**: Evaluates model performance on test data using CPU instances (ml.m5.xlarge)\n",
    "3. **Conditional Registration**: Registers model based on text similarity scores from CPU evaluation\n",
    "4. **Automated Workflow**: All steps are orchestrated automatically with cost-effective evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry - Register Model Based on Performance\n",
    "\n",
    "This notebook registers the fine-tuned model to SageMaker Model Registry if performance criteria are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::637423390840:role/WSParticipantRole\" # need to change your role\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Clients\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "# S3 Bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"qwen3-0-6-lora-samples\"\n",
    "\n",
    "print(f\"Using bucket: {bucket}\")\n",
    "print(f\"Using region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the processing job name from previous notebook\n",
    "processing_job_name = \"qwen3-evaluation-2025-08-31-12-41-53\"  # UPDATE THIS\n",
    "\n",
    "# Load evaluation metrics from S3\n",
    "metrics_key = f\"{prefix}/evaluation/{processing_job_name}/evaluation_metrics.json\"\n",
    "\n",
    "try:\n",
    "    response = s3_client.get_object(Bucket=bucket, Key=metrics_key)\n",
    "    evaluation_metrics = json.loads(response['Body'].read())\n",
    "    \n",
    "    print(\"=== Current Model Performance ===\")\n",
    "    \n",
    "    # Handle different metric formats (CPU vs GPU evaluation)\n",
    "    if 'eval_loss' in evaluation_metrics:\n",
    "        # Original GPU-based evaluation format\n",
    "        print(f\"Evaluation Loss: {evaluation_metrics['eval_loss']:.4f}\")\n",
    "        print(f\"BLEU Score: {evaluation_metrics['bleu_score']:.4f}\")\n",
    "    elif 'text_similarity' in evaluation_metrics:\n",
    "        # CPU-based evaluation format\n",
    "        print(f\"Text Similarity: {evaluation_metrics['text_similarity']:.4f}\")\n",
    "        print(f\"Average Prediction Length: {evaluation_metrics['avg_prediction_length']:.2f} words\")\n",
    "        print(f\"Average Reference Length: {evaluation_metrics['avg_reference_length']:.2f} words\")\n",
    "        print(f\"Evaluation Type: {evaluation_metrics.get('evaluation_type', 'N/A')}\")\n",
    "        \n",
    "        # Convert to expected format for compatibility\n",
    "        evaluation_metrics['eval_loss'] = 1.0 - evaluation_metrics['text_similarity']  # Convert similarity to loss-like metric\n",
    "        evaluation_metrics['bleu_score'] = evaluation_metrics['text_similarity'] * 0.5  # Approximate BLEU from similarity\n",
    "        \n",
    "        print(f\"\\n=== Converted Metrics for Registry ===\")\n",
    "        print(f\"Derived Eval Loss: {evaluation_metrics['eval_loss']:.4f}\")\n",
    "        print(f\"Derived BLEU Score: {evaluation_metrics['bleu_score']:.4f}\")\n",
    "    else:\n",
    "        print(\"Unknown metrics format\")\n",
    "    \n",
    "    print(f\"Number of test samples: {evaluation_metrics['num_samples']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading metrics: {e}\")\n",
    "    print(\"Please ensure the evaluation job has completed successfully\")\n",
    "    evaluation_metrics = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Performance Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define performance thresholds for model registration - adapted for CPU evaluation\n",
    "if evaluation_metrics and 'text_similarity' in evaluation_metrics:\n",
    "    # CPU-based evaluation thresholds\n",
    "    EVAL_LOSS_THRESHOLD = 10  # Maximum acceptable loss (1 - text_similarity)\n",
    "    BLEU_SCORE_THRESHOLD = 0.00  # Minimum acceptable derived BLEU score\n",
    "    \n",
    "    # Baseline metrics for CPU evaluation\n",
    "    baseline_metrics = {\n",
    "        \"eval_loss\": 10,  # Example baseline\n",
    "        \"bleu_score\": 0.00  # Example baseline\n",
    "    }\n",
    "else:\n",
    "    # Original GPU-based evaluation thresholds\n",
    "    EVAL_LOSS_THRESHOLD = 10  # Maximum acceptable loss\n",
    "    BLEU_SCORE_THRESHOLD = 0  # Minimum acceptable BLEU score\n",
    "    \n",
    "    # Check baseline metrics (optional - for comparison with previous models)\n",
    "    baseline_metrics = {\n",
    "        \"eval_loss\": 11,  # Example baseline\n",
    "        \"bleu_score\": 0.00  # Example baseline\n",
    "    }\n",
    "\n",
    "print(f\"\\n=== Performance Thresholds ===\")\n",
    "print(f\"Max Eval Loss: {EVAL_LOSS_THRESHOLD}\")\n",
    "print(f\"Min BLEU Score: {BLEU_SCORE_THRESHOLD}\")\n",
    "print(f\"\\nBaseline Eval Loss: {baseline_metrics['eval_loss']}\")\n",
    "print(f\"Baseline BLEU Score: {baseline_metrics['bleu_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if model meets performance criteria\n",
    "def check_model_performance(metrics, thresholds, baseline=None):\n",
    "    \"\"\"\n",
    "    Check if model meets performance criteria.\n",
    "    Returns: (should_register, reasons)\n",
    "    \"\"\"\n",
    "    reasons = []\n",
    "    should_register = True\n",
    "    \n",
    "    # Check absolute thresholds\n",
    "    if metrics['eval_loss'] > thresholds['max_loss']:\n",
    "        should_register = False\n",
    "        reasons.append(f\"Eval loss {metrics['eval_loss']:.4f} exceeds threshold {thresholds['max_loss']}\")\n",
    "    else:\n",
    "        reasons.append(f\"✓ Eval loss {metrics['eval_loss']:.4f} meets threshold\")\n",
    "    \n",
    "    if metrics['bleu_score'] < thresholds['min_bleu']:\n",
    "        should_register = False\n",
    "        reasons.append(f\"BLEU score {metrics['bleu_score']:.4f} below threshold {thresholds['min_bleu']}\")\n",
    "    else:\n",
    "        reasons.append(f\"✓ BLEU score {metrics['bleu_score']:.4f} meets threshold\")\n",
    "    \n",
    "    # Check improvement over baseline\n",
    "    if baseline:\n",
    "        if metrics['eval_loss'] < baseline['eval_loss']:\n",
    "            reasons.append(f\"✓ Improved loss: {metrics['eval_loss']:.4f} < {baseline['eval_loss']:.4f}\")\n",
    "        else:\n",
    "            reasons.append(f\"No improvement in loss over baseline\")\n",
    "        \n",
    "        if metrics['bleu_score'] > baseline['bleu_score']:\n",
    "            reasons.append(f\"✓ Improved BLEU: {metrics['bleu_score']:.4f} > {baseline['bleu_score']:.4f}\")\n",
    "        else:\n",
    "            reasons.append(f\"No improvement in BLEU over baseline\")\n",
    "    \n",
    "    return should_register, reasons\n",
    "\n",
    "if evaluation_metrics:\n",
    "    thresholds = {\n",
    "        'max_loss': EVAL_LOSS_THRESHOLD,\n",
    "        'min_bleu': BLEU_SCORE_THRESHOLD\n",
    "    }\n",
    "    \n",
    "    should_register, reasons = check_model_performance(\n",
    "        evaluation_metrics, \n",
    "        thresholds, \n",
    "        baseline_metrics\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n=== Performance Check Results ===\")\n",
    "    for reason in reasons:\n",
    "        print(reason)\n",
    "    \n",
    "    print(f\"\\\\nDecision: {'REGISTER MODEL ✓' if should_register else 'DO NOT REGISTER ✗'}\")\n",
    "else:\n",
    "    should_register = False\n",
    "    print(\"Cannot evaluate performance - metrics not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model package group name\n",
    "model_package_group_name = \"qwen3-0-6b-lora-models\"\n",
    "\n",
    "# Create model package group if it doesn't exist\n",
    "try:\n",
    "    response = sm_client.describe_model_package_group(\n",
    "        ModelPackageGroupName=model_package_group_name\n",
    "    )\n",
    "    print(f\"Model package group '{model_package_group_name}' already exists\")\n",
    "except Exception as e:\n",
    "    # Check if it's because the group doesn't exist\n",
    "    if 'does not exist' in str(e) or 'ResourceNotFound' in str(e.__class__.__name__):\n",
    "        print(f\"Creating model package group '{model_package_group_name}'...\")\n",
    "        response = sm_client.create_model_package_group(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelPackageGroupDescription=\"Fine-tuned QWEN3-0.6B models with LoRA\",\n",
    "            Tags=[\n",
    "                {'Key': 'Project', 'Value': 'MLOps-Workshop'},\n",
    "                {'Key': 'Model', 'Value': 'QWEN3-0.6B'},\n",
    "                {'Key': 'Technique', 'Value': 'LoRA'}\n",
    "            ]\n",
    "        )\n",
    "        print(f\"Model package group created: {response['ModelPackageGroupArn']}\")\n",
    "    else:\n",
    "        # Re-raise if it's a different error\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Register Model to Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if should_register and evaluation_metrics:\n",
    "    # Get training job details\n",
    "    training_job_name = \"qwen3-0-6b-lora-fine-tuning-lora-2025-08-31-12-28-29\"\n",
    "    training_job = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    model_artifacts_uri = training_job['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    # Get the container image URI for PyTorch\n",
    "    from sagemaker import image_uris\n",
    "    inference_image = image_uris.retrieve(\n",
    "        framework=\"pytorch\",\n",
    "        region=region,\n",
    "        version=\"2.6.0\",  # Use stable version\n",
    "        py_version=\"py312\",\n",
    "        instance_type=\"ml.g5.2xlarge\",\n",
    "        image_scope=\"inference\"\n",
    "    )\n",
    "    \n",
    "    # Create inference specification\n",
    "    inference_spec = {\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": inference_image,\n",
    "                \"ModelDataUrl\": model_artifacts_uri,\n",
    "                \"Environment\": {\n",
    "                    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "                    \"SAGEMAKER_SUBMIT_DIRECTORY\": f\"{model_artifacts_uri}/code\",\n",
    "                    \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "                    \"SAGEMAKER_REGION\": region,\n",
    "                    \"MODEL_NAME\": \"Qwen/Qwen3-0.6B\",\n",
    "                    \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedTransformInstanceTypes\": [\"ml.g5.xlarge\", \"ml.g5.2xlarge\", \"ml.g5.4xlarge\"],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": [\"ml.g5.xlarge\", \"ml.g5.2xlarge\", \"ml.g5.4xlarge\"],\n",
    "        \"SupportedContentTypes\": [\"application/json\"],\n",
    "        \"SupportedResponseMIMETypes\": [\"application/json\"]\n",
    "    }\n",
    "    \n",
    "    # Model package version description - include metrics info in description\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_description = f\"QWEN3-0.6B LoRA fine-tuned model | Training Job: {training_job_name} | Eval Loss: {evaluation_metrics['eval_loss']:.4f} | BLEU: {evaluation_metrics['bleu_score']:.4f} | Samples: {evaluation_metrics['num_samples']} | Timestamp: {timestamp}\"\n",
    "    \n",
    "    # Create model package without Tags (Tags should be on Model Package Group, not versions)\n",
    "    model_package_response = sm_client.create_model_package(\n",
    "        ModelPackageGroupName=model_package_group_name,\n",
    "        ModelPackageDescription=model_description,\n",
    "        InferenceSpecification=inference_spec,\n",
    "        ModelApprovalStatus=\"PendingManualApproval\",  # Start with pending approval\n",
    "        ModelMetrics={\n",
    "            \"ModelQuality\": {\n",
    "                \"Statistics\": {\n",
    "                    \"ContentType\": \"application/json\",\n",
    "                    \"S3Uri\": f\"s3://{bucket}/{prefix}/evaluation/{processing_job_name}/evaluation_metrics.json\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model_package_arn = model_package_response['ModelPackageArn']\n",
    "    print(f\"\\n=== Model Registration Successful ===\")\n",
    "    print(f\"Model Package ARN: {model_package_arn}\")\n",
    "    print(f\"Status: PendingManualApproval\")\n",
    "    \n",
    "    # Store ARN for later use\n",
    "    registered_model_package_arn = model_package_arn\n",
    "    \n",
    "else:\n",
    "    print(\"\\n=== Model Not Registered ===\")\n",
    "    print(\"Model does not meet performance criteria or metrics unavailable\")\n",
    "    registered_model_package_arn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Approve Model (Optional - Manual Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Approve the model for deployment\n",
    "if registered_model_package_arn:\n",
    "    approve_model = input(\"Do you want to approve this model for deployment? (yes/no): \")\n",
    "    \n",
    "    if approve_model.lower() == 'yes':\n",
    "        response = sm_client.update_model_package(\n",
    "            ModelPackageArn=registered_model_package_arn,\n",
    "            ModelApprovalStatus=\"Approved\",\n",
    "            ApprovalDescription=\"Model meets performance criteria and approved for deployment\"\n",
    "        )\n",
    "        print(\"\\\\n✓ Model approved for deployment\")\n",
    "    else:\n",
    "        print(\"\\\\nModel remains in PendingManualApproval status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. List Registered Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all model packages in the group\n",
    "response = sm_client.list_model_packages(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    "    MaxResults=10\n",
    ")\n",
    "\n",
    "print(f\"\\\\n=== Models in {model_package_group_name} ===\")\n",
    "for idx, model_package in enumerate(response['ModelPackageSummaryList'], 1):\n",
    "    print(f\"\\\\n{idx}. Model Package:\")\n",
    "    print(f\"   ARN: {model_package['ModelPackageArn']}\")\n",
    "    print(f\"   Status: {model_package['ModelApprovalStatus']}\")\n",
    "    print(f\"   Created: {model_package['CreationTime']}\")\n",
    "    \n",
    "    # Get detailed metrics if available\n",
    "    try:\n",
    "        details = sm_client.describe_model_package(\n",
    "            ModelPackageName=model_package['ModelPackageArn']\n",
    "        )\n",
    "        if 'ModelPackageDescription' in details:\n",
    "            print(f\"   Description: {details['ModelPackageDescription'][:100]}...\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Model Package ARN for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export the latest approved model package ARN for deployment\n",
    "approved_models = []\n",
    "for model_package in response['ModelPackageSummaryList']:\n",
    "    if model_package['ModelApprovalStatus'] == 'Approved':\n",
    "        approved_models.append(model_package['ModelPackageArn'])\n",
    "\n",
    "if approved_models:\n",
    "    latest_approved_model_arn = approved_models[0]\n",
    "    print(f\"\\\\n=== Latest Approved Model ===\")\n",
    "    print(f\"ARN: {latest_approved_model_arn}\")\n",
    "    print(\"\\\\nThis model is ready for deployment to an endpoint.\")\n",
    "    \n",
    "    # Save to file for next notebook\n",
    "    with open('latest_model_arn.txt', 'w') as f:\n",
    "        f.write(latest_approved_model_arn)\n",
    "    print(\"Model ARN saved to 'latest_model_arn.txt'\")\n",
    "else:\n",
    "    print(\"\\\\nNo approved models found. Please approve a model before deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

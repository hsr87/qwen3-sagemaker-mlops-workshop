{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation with SageMaker Processing Job\n",
    "\n",
    "This notebook evaluates the fine-tuned QWEN3-0.6B model using SageMaker Processing Job on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.pytorch import PyTorchProcessor\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure SageMaker Session and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::637423390840:role/WSParticipantRole\" # need to change your role\n",
    "\n",
    "# S3 Bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"qwen3-0-6-lora-samples\"\n",
    "\n",
    "# Training job name from previous notebook - UPDATE THIS\n",
    "training_job_name = \"qwen3-0-6b-lora-fine-tuning-lora-2025-08-31-12-28-29\"\n",
    "\n",
    "print(f\"Using bucket: {bucket}\")\n",
    "print(f\"Using prefix: {prefix}\")\n",
    "print(f\"Evaluating model from training job: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Test Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload test.jsonl data to S3\n",
    "print(\"Uploading test.jsonl data to S3...\")\n",
    "test_s3_uri = sagemaker_session.upload_data(\n",
    "    path='samples/test.jsonl',\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/data/test'\n",
    ")\n",
    "print(f\"Test data uploaded to: {test_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Model S3 Location from Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get model artifacts location from training job\n",
    "sm_client = boto3.client('sagemaker')\n",
    "training_job = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_s3_uri = training_job['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Model artifacts location: {model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create evaluation script directory\n",
    "os.makedirs('src/evaluation', exist_ok=True)\n",
    "\n",
    "# Write evaluation script - CPU optimized version\n",
    "evaluation_script = '''#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import tarfile\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=256):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                self.data.append(item)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item.get(\"text\", \"\")\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"text\": text\n",
    "        }\n",
    "\n",
    "def evaluate_model():\n",
    "    # Paths\n",
    "    model_path = \"/opt/ml/processing/model\"\n",
    "    test_data_path = \"/opt/ml/processing/input/test/test.jsonl\"\n",
    "    output_path = \"/opt/ml/processing/output\"\n",
    "    \n",
    "    # Extract model.tar.gz\n",
    "    print(\"Extracting model artifacts...\")\n",
    "    with tarfile.open(os.path.join(model_path, \"model.tar.gz\"), \"r:gz\") as tar:\n",
    "        tar.extractall(model_path)\n",
    "    \n",
    "    # Load tokenizer and base model - CPU optimized\n",
    "    print(\"Loading tokenizer and base model...\")\n",
    "    model_name = \"Qwen/Qwen3-0.6B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load base model for CPU\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,  # Changed to float32 for CPU\n",
    "        device_map=\"cpu\",  # Force CPU usage\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create test dataset\n",
    "    print(\"Loading test data...\")\n",
    "    test_dataset = TestDataset(test_data_path, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Evaluation - CPU based\n",
    "    print(\"Starting evaluation...\")\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"]  # No .cuda() for CPU\n",
    "            attention_mask = batch[\"attention_mask\"]  # No .cuda() for CPU\n",
    "            \n",
    "            # Generate predictions with reduced parameters for CPU\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=64,  # Reduced for CPU performance\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            # Decode predictions\n",
    "            pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            original_text = batch[\"text\"][0]\n",
    "            \n",
    "            # Extract generated part (simple approach)\n",
    "            if len(pred_text) > len(original_text):\n",
    "                generated_text = pred_text[len(original_text):].strip()\n",
    "            else:\n",
    "                generated_text = pred_text\n",
    "            \n",
    "            all_predictions.append(generated_text)\n",
    "            all_references.append(original_text)\n",
    "            \n",
    "            # Calculate perplexity instead of loss for CPU efficiency\n",
    "            num_samples += 1\n",
    "    \n",
    "    # Calculate simple metrics\n",
    "    avg_pred_length = np.mean([len(pred.split()) for pred in all_predictions])\n",
    "    avg_ref_length = np.mean([len(ref.split()) for ref in all_references])\n",
    "    \n",
    "    # Simple text similarity metric\n",
    "    def calculate_similarity(predictions, references):\n",
    "        similarities = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_words = set(pred.lower().split())\n",
    "            ref_words = set(ref.lower().split())\n",
    "            \n",
    "            if len(ref_words) == 0:\n",
    "                similarities.append(0)\n",
    "                continue\n",
    "            \n",
    "            intersection = pred_words & ref_words\n",
    "            similarity = len(intersection) / len(ref_words) if len(ref_words) > 0 else 0\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        return np.mean(similarities)\n",
    "    \n",
    "    text_similarity = calculate_similarity(all_predictions, all_references)\n",
    "    \n",
    "    # Prepare results\n",
    "    metrics = {\n",
    "        \"text_similarity\": text_similarity,\n",
    "        \"avg_prediction_length\": avg_pred_length,\n",
    "        \"avg_reference_length\": avg_ref_length,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"model_location\": model_path,\n",
    "        \"evaluation_type\": \"cpu_based\"\n",
    "    }\n",
    "    \n",
    "    # Save metrics\n",
    "    print(f\"\\\\nEvaluation Results:\")\n",
    "    print(f\"Text Similarity: {text_similarity:.4f}\")\n",
    "    print(f\"Average Prediction Length: {avg_pred_length:.2f} words\")\n",
    "    print(f\"Average Reference Length: {avg_ref_length:.2f} words\")\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    \n",
    "    # Save results to JSON\n",
    "    with open(os.path.join(output_path, \"evaluation_metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    # Save sample predictions\n",
    "    samples = []\n",
    "    for i in range(min(5, len(all_predictions))):\n",
    "        samples.append({\n",
    "            \"prediction\": all_predictions[i],\n",
    "            \"reference\": all_references[i]\n",
    "        })\n",
    "    \n",
    "    with open(os.path.join(output_path, \"sample_predictions.json\"), \"w\") as f:\n",
    "        json.dump(samples, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\\\nEvaluation completed successfully!\")\n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_model()\n",
    "'''\n",
    "\n",
    "with open('src/evaluation/evaluate.py', 'w') as f:\n",
    "    f.write(evaluation_script)\n",
    "\n",
    "print(\"CPU-optimized evaluation script created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create and Run Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create PyTorchProcessor - CPU based evaluation to avoid g5 quota issues\n",
    "processor = PyTorchProcessor(\n",
    "    framework_version=\"2.6.0\",\n",
    "    py_version=\"py312\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",  # Changed from ml.g5.2xlarge to avoid quota issues\n",
    "    instance_count=1,\n",
    "    base_job_name=\"qwen3-evaluation\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Job name\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "processing_job_name = f\"qwen3-evaluation-{timestamp}\"\n",
    "\n",
    "# Run processing job\n",
    "print(f\"Starting processing job: {processing_job_name}\")\n",
    "processor.run(\n",
    "    code=\"evaluate.py\",\n",
    "    source_dir=\"src/evaluation\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=model_s3_uri,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "            input_name=\"model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=test_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input/test\",\n",
    "            input_name=\"test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"s3://{bucket}/{prefix}/evaluation/{processing_job_name}\",\n",
    "            output_name=\"evaluation_results\"\n",
    "        )\n",
    "    ],\n",
    "    job_name=processing_job_name,\n",
    "    wait=False\n",
    ")\n",
    "\n",
    "print(f\"\\\\nProcessing job '{processing_job_name}' has been submitted!\")\n",
    "print(f\"Evaluation results will be saved to: s3://{bucket}/{prefix}/evaluation/{processing_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor Processing Job and Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wait for job completion (optional)\n",
    "# processor.jobs[-1].wait()\n",
    "\n",
    "# After job completes, download and display results\n",
    "import time\n",
    "print(\"Waiting for processing job to complete...\")\n",
    "print(\"This may take 10-15 minutes...\")\n",
    "\n",
    "# You can monitor the job status\n",
    "while True:\n",
    "    job_description = sm_client.describe_processing_job(ProcessingJobName=processing_job_name)\n",
    "    status = job_description['ProcessingJobStatus']\n",
    "    print(f\"Job status: {status}\")\n",
    "    \n",
    "    if status in ['Completed', 'Failed', 'Stopped']:\n",
    "        break\n",
    "    \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load and Display Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download evaluation results\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "if status == 'Completed':\n",
    "    # Download metrics\n",
    "    metrics_key = f\"{prefix}/evaluation/{processing_job_name}/evaluation_metrics.json\"\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=metrics_key)\n",
    "        metrics = json.loads(response['Body'].read())\n",
    "        \n",
    "        print(\"\\\\n=== Evaluation Results ===\")\n",
    "        print(f\"Evaluation Loss: {metrics['text_similarity']:.4f}\")\n",
    "        print(f\"Number of test samples: {metrics['num_samples']}\")\n",
    "        \n",
    "        # Store metrics for model registry decision\n",
    "        evaluation_metrics = metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metrics: {e}\")\n",
    "else:\n",
    "    print(f\"Processing job failed with status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

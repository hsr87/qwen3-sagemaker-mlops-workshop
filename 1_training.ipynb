{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QWEN3-0.6B LoRA Fine-tuning on SageMaker\n",
    "\n",
    "This notebook demonstrates how to fine-tune QWEN3-0.6B using LoRA on Amazon SageMaker with local sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker # to use pytorch 2.7.1 for training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure SageMaker Session and Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# IAM role\n",
    "role = \"arn:aws:iam::637423390840:role/WSParticipantRole\" # need to change your role\n",
    "print(f\"Using SageMaker execution role: {role}\")\n",
    "\n",
    "# S3 Bucket (default bucket)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"qwen3-0-6-lora-samples\"\n",
    "\n",
    "print(f\"Using bucket: {bucket}\")\n",
    "print(f\"Using prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Local Sample Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload local train.jsonl data to S3\n",
    "print(\"Uploading train.jsonl data to S3...\")\n",
    "train_s3_uri = sagemaker_session.upload_data(\n",
    "    path='samples/train.jsonl',\n",
    "    bucket=bucket,\n",
    "    key_prefix=f'{prefix}/data/train'\n",
    ")\n",
    "print(f\"Training data uploaded to: {train_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "exp_name = 'qwen3-0-6b-lora-fine-tuning'\n",
    "instance_type = 'ml.g5.2xlarge'\n",
    "\n",
    "# Output paths\n",
    "output_path = f\"s3://{bucket}/{prefix}/output\"\n",
    "checkpoint_s3_uri = f\"s3://{bucket}/{prefix}/checkpoints\"\n",
    "\n",
    "# Job name based on timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "job_name = f\"{exp_name}-lora-{timestamp}\"\n",
    "\n",
    "print(f\"Job name: {job_name}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"Checkpoint path: {checkpoint_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "hyperparameters = {\n",
    "    # Model\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \n",
    "    # Training (HuggingFace TrainingArguments)\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 64,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"save_steps\": 50,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_total_limit\": 3,\n",
    "    \"do_eval\": True,  # Enable evaluation\n",
    "    \"eval_strategy\": \"steps\",  # Use eval_strategy instead of evaluation_strategy\n",
    "    \"eval_steps\": 50,\n",
    "    \"metric_for_best_model\": \"eval_loss\",\n",
    "    \"greater_is_better\": False,\n",
    "    \"load_best_model_at_end\": False,  # Disable for LoRA\n",
    "    \"report_to\": \"none\",\n",
    "    \"bf16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    # DeepSpeed removed - not using distributed training\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 4, \n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\",\n",
    "    \n",
    "    # Dataset\n",
    "    \"train_file\": \"/opt/ml/input/data/train/train.jsonl\",  # SageMaker mounts S3 data here\n",
    "    \"validation_split_percentage\": 20,  # Split 20% for validation from train.jsonl\n",
    "    \"block_size\": 256,\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create PyTorch Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch Estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\", # entry point code\n",
    "    source_dir=\"src\",  # source directory\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"2.7.1\",  \n",
    "    py_version=\"py312\",  \n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=output_path,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=False,  \n",
    "    max_run=24*60*60,  # Maximum 24 hours\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    volume_size=450,\n",
    "    environment={\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"PyTorch estimator created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Training Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training data input - using train.jsonl\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_s3_uri,\n",
    "    content_type=\"application/jsonl\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    distribution=\"FullyReplicated\"\n",
    ")\n",
    "\n",
    "print(f\"Training input configured with data from: {train_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Start Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training job\n",
    "print(f\"Starting training job: {job_name}\")\n",
    "print(f\"Training data: {train_s3_uri}\")\n",
    "print(f\"Output path: {output_path}\")\n",
    "print(f\"Note: The training script will automatically split train.jsonl - {100-hyperparameters['validation_split_percentage']}% for training, {hyperparameters['validation_split_percentage']}% for validation\")\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": train_input\n",
    "    },\n",
    "    job_name=job_name,\n",
    "    wait=False  # Asynchronous start\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining job '{job_name}' has been submitted!\")\n",
    "print(f\"You can monitor the job in the SageMaker console\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
